# Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ø¬Ø§Ù…Ø¹ Ù…ÙˆØªÙˆØ± ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ ØªÙ‚ÙˆÛŒØªÛŒ (RL Engine)
# Comprehensive Guide to Reinforcement Learning Engine

**SecureRedLab - Academic Research Platform**  
**Ù†Ø³Ø®Ù‡:** 1.0.0  
**ØªØ§Ø±ÛŒØ®:** 2025-01-15  

---

## ğŸ“‹ ÙÙ‡Ø±Ø³Øª Ù…Ø·Ø§Ù„Ø¨

1. [Ù…Ø¹Ø±ÙÛŒ](#Ù…Ø¹Ø±ÙÛŒ)
2. [Ù…Ø¹Ù…Ø§Ø±ÛŒ Ú©Ù„ÛŒ](#Ù…Ø¹Ù…Ø§Ø±ÛŒ-Ú©Ù„ÛŒ)
3. [Ù…ÙØ§Ù‡ÛŒÙ… Ù¾Ø§ÛŒÙ‡ RL](#Ù…ÙØ§Ù‡ÛŒÙ…-Ù¾Ø§ÛŒÙ‡-rl)
4. [Ù†Ø­ÙˆÙ‡ Ø§Ø³ØªÙØ§Ø¯Ù‡](#Ù†Ø­ÙˆÙ‡-Ø§Ø³ØªÙØ§Ø¯Ù‡)
5. [Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ](#Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ)
6. [Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡](#Ù¾Ø§ÛŒÚ¯Ø§Ù‡-Ø¯Ø§Ø¯Ù‡)
7. [Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§](#Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…Ù‡Ø§)
8. [Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ](#Ø¨Ù‡ÛŒÙ†Ù‡Ø³Ø§Ø²ÛŒ)
9. [Ù…Ø«Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø±Ø¯ÛŒ](#Ù…Ø«Ø§Ù„Ù‡Ø§ÛŒ-Ú©Ø§Ø±Ø¨Ø±Ø¯ÛŒ)
10. [Ø¹ÛŒØ¨â€ŒÛŒØ§Ø¨ÛŒ](#Ø¹ÛŒØ¨ÛŒØ§Ø¨ÛŒ)

---

## ğŸ¯ Ù…Ø¹Ø±ÙÛŒ

Ù…ÙˆØªÙˆØ± ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ ØªÙ‚ÙˆÛŒØªÛŒ (RL Engine) **Ù‚Ù„Ø¨** Ø³ÛŒØ³ØªÙ… SecureRedLab Ø§Ø³Øª Ú©Ù‡ Ø¨Ù‡ Ù¾Ù„ØªÙØ±Ù… Ø§Ø¬Ø§Ø²Ù‡ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯:

- ğŸ“ˆ **ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø§Ø² ØªØ¬Ø±Ø¨Ù‡**: Ù¾Ø³ Ø§Ø² Ù‡Ø± ØªØ³ØªØŒ Ø§Ø² Ù†ØªØ§ÛŒØ¬ ÛŒØ§Ø¯ Ø¨Ú¯ÛŒØ±Ø¯
- ğŸ¯ **Ø¨Ù‡Ø¨ÙˆØ¯ Ø®ÙˆØ¯Ú©Ø§Ø±**: Ú©ÛŒÙÛŒØª Ø­Ù…Ù„Ø§Øª Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ø®ÙˆØ¯Ú©Ø§Ø± Ø§ÙØ²Ø§ÛŒØ´ Ø¯Ù‡Ø¯
- ğŸ§  **ØªØµÙ…ÛŒÙ…â€ŒÚ¯ÛŒØ±ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯**: Ø¨Ù‡ØªØ±ÛŒÙ† Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù‡Ø¯Ù Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†Ø¯
- ğŸ’¾ **Ø°Ø®ÛŒØ±Ù‡ ØªØ¬Ø±Ø¨ÛŒØ§Øª**: ØªÙ…Ø§Ù… ØªØ¬Ø±Ø¨ÛŒØ§Øª Ø±Ø§ Ø¯Ø± Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ Ù†Ú¯Ù‡Ø¯Ø§Ø±ÛŒ Ú©Ù†Ø¯
- ğŸ”„ **Ø¨Ø§Ø²Ø¢Ù…ÙˆØ²ÛŒ Ù…Ø¯Ø§ÙˆÙ…**: Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ø¯ÙˆØ±Ù‡â€ŒØ§ÛŒ Ø¨Ø§Ø²Ø¢Ù…ÙˆØ²ÛŒ Ø¯Ù‡Ø¯

### ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ú©Ù„ÛŒØ¯ÛŒ

âœ… **5 Agent Ù…Ø³ØªÙ‚Ù„**: DDoS, Shell Upload, Data Extraction, Deface, Behavior Simulation  
âœ… **4 Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… RL**: Q-Learning, Deep Q-Network (DQN), Policy Gradient, PPO  
âœ… **Experience Replay Buffer**: Ø¨Ø§ Priority Sampling Ø¨Ø±Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¨Ù‡ØªØ±  
âœ… **Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ Ø¨Ø§Ø²Ø¢Ù…ÙˆØ²ÛŒ**: Ø°Ø®ÛŒØ±Ù‡ Ùˆ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØªØ¬Ø±Ø¨ÛŒØ§Øª Ø§Ø² PostgreSQL  
âœ… **Reward Shaping**: ØªØ§Ø¨Ø¹ Ù¾Ø§Ø¯Ø§Ø´ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¨Ø§ ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ Ù‚Ø§Ø¨Ù„ ØªÙ†Ø¸ÛŒÙ…  
âœ… **Model Versioning**: Ù†Ø³Ø®Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ùˆ A/B Testing  
âœ… **Multi-threaded Training**: Ø¢Ù…ÙˆØ²Ø´ Ø³Ø±ÛŒØ¹ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú†Ù†Ø¯ thread  

---

## ğŸ—ï¸ Ù…Ø¹Ù…Ø§Ø±ÛŒ Ú©Ù„ÛŒ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SecureRedLab Reinforcement Learning Engine                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  RLEngineManager (Singleton)                        â”‚   â”‚
â”‚  â”‚  - Ù…Ø¯ÛŒØ±ÛŒØª ØªÙ…Ø§Ù… AgentÙ‡Ø§                             â”‚   â”‚
â”‚  â”‚  - Ù…Ø¯ÛŒØ±ÛŒØª Replay Buffers                            â”‚   â”‚
â”‚  â”‚  - Ù…Ø¯ÛŒØ±ÛŒØª Ø¨Ø§Ø²Ø¢Ù…ÙˆØ²ÛŒ                                  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚              â”‚                                              â”‚
â”‚              â–¼                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  5 Independent RL Agents                            â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚   â”‚
â”‚  â”‚  â”‚  DDoS     â”‚  â”‚  Shell    â”‚  â”‚  Extract  â”‚       â”‚   â”‚
â”‚  â”‚  â”‚  Agent    â”‚  â”‚  Agent    â”‚  â”‚  Agent    â”‚       â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚   â”‚
â”‚  â”‚  â”‚  Deface   â”‚  â”‚  Behavior â”‚                      â”‚   â”‚
â”‚  â”‚  â”‚  Agent    â”‚  â”‚  Agent    â”‚                      â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚              â”‚                                              â”‚
â”‚              â–¼                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Experience Replay Buffers (100K capacity each)     â”‚   â”‚
â”‚  â”‚  - Priority Experience Replay                       â”‚   â”‚
â”‚  â”‚  - Importance Sampling                              â”‚   â”‚
â”‚  â”‚  - Save/Load from Database                          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚              â”‚                                              â”‚
â”‚              â–¼                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  PostgreSQL Database                                â”‚   â”‚
â”‚  â”‚  - rl_experiences (ØªØ¬Ø±Ø¨ÛŒØ§Øª)                         â”‚   â”‚
â”‚  â”‚  - rl_episodes (Ù†ØªØ§ÛŒØ¬ Episode)                      â”‚   â”‚
â”‚  â”‚  - rl_models (Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´â€ŒØ¯ÛŒØ¯Ù‡)                   â”‚   â”‚
â”‚  â”‚  - rl_agent_stats (Ø¢Ù…Ø§Ø±)                            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“š Ù…ÙØ§Ù‡ÛŒÙ… Ù¾Ø§ÛŒÙ‡ RL

### 1. State (ÙˆØ¶Ø¹ÛŒØª)

State ØªÙ…Ø§Ù… Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù„Ø§Ø²Ù… Ø¨Ø±Ø§ÛŒ ØªØµÙ…ÛŒÙ…â€ŒÚ¯ÛŒØ±ÛŒ Ø±Ø§ Ø´Ø§Ù…Ù„ Ù…ÛŒâ€ŒØ´ÙˆØ¯:

```python
state = RLState(
    target_ip="192.168.1.100",      # IP Ù‡Ø¯Ù
    target_ports=[80, 443],          # Ù¾ÙˆØ±Øªâ€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ø²
    target_os="Linux",               # Ø³ÛŒØ³ØªÙ…â€ŒØ¹Ø§Ù…Ù„
    target_services={...},           # Ø³Ø±ÙˆÛŒØ³â€ŒÙ‡Ø§ÛŒ Ø¯Ø± Ø­Ø§Ù„ Ø§Ø¬Ø±Ø§
    network_latency=50.0,            # ØªØ£Ø®ÛŒØ± Ø´Ø¨Ú©Ù‡
    bandwidth=1000.0,                # Ù¾Ù‡Ù†Ø§ÛŒ Ø¨Ø§Ù†Ø¯
    firewall_active=True,            # Ø¢ÛŒØ§ ÙØ§ÛŒØ±ÙˆØ§Ù„ ÙØ¹Ø§Ù„ Ø§Ø³ØªØŸ
    ids_active=True,                 # Ø¢ÛŒØ§ IDS ÙØ¹Ø§Ù„ Ø§Ø³ØªØŸ
    attack_stage=0,                  # Ù…Ø±Ø­Ù„Ù‡ ÙØ¹Ù„ÛŒ Ø­Ù…Ù„Ù‡
    time_elapsed=0.0,                # Ø²Ù…Ø§Ù† Ø³Ù¾Ø±ÛŒ Ø´Ø¯Ù‡
    packets_sent=0,                  # ØªØ¹Ø¯Ø§Ø¯ Ù¾Ú©Øª Ø§Ø±Ø³Ø§Ù„ÛŒ
    success_rate=0.0,                # Ù†Ø±Ø® Ù…ÙˆÙÙ‚ÛŒØª ÙØ¹Ù„ÛŒ
    previous_actions=[],             # Ø§Ù‚Ø¯Ø§Ù…Ø§Øª Ù‚Ø¨Ù„ÛŒ
    detection_count=0                # Ø¯ÙØ¹Ø§Øª Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ
)
```

State Ø¨Ù‡ ØµÙˆØ±Øª Ø®ÙˆØ¯Ú©Ø§Ø± Ø¨Ù‡ ÛŒÚ© Ø¨Ø±Ø¯Ø§Ø± 13 Ø¨Ø¹Ø¯ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ù…ÛŒâ€ŒØ´ÙˆØ¯:

```python
state_vector = state.to_vector()
# Output: [0.02, 0.5, 0.06, 0.05, 0.1, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
```

### 2. Action (Ø¹Ù…Ù„)

Action ØªØµÙ…ÛŒÙ…ÛŒ Ø§Ø³Øª Ú©Ù‡ Agent Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯:

```python
action = RLAction(
    action_type="increase_bot_power",  # Ù†ÙˆØ¹ Ø¹Ù…Ù„
    parameters={                       # Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø¹Ù…Ù„
        'intensity': 0.8,
        'target_port': 80,
        'packet_size': 1500
    }
)
```

Ø§Ù†ÙˆØ§Ø¹ Action Ø¨Ø±Ø§ÛŒ Ù‡Ø± Agent Ù…ØªÙØ§ÙˆØª Ø§Ø³Øª:

**DDoS Agent:**
- `increase_bot_power`: Ø§ÙØ²Ø§ÛŒØ´ Ù‚Ø¯Ø±Øª botÙ‡Ø§
- `decrease_bot_power`: Ú©Ø§Ù‡Ø´ Ù‚Ø¯Ø±Øª botÙ‡Ø§
- `change_attack_vector`: ØªØºÛŒÛŒØ± Ø¨Ø±Ø¯Ø§Ø± Ø­Ù…Ù„Ù‡
- `add_bots`: Ø§ÙØ²ÙˆØ¯Ù† bot Ø¬Ø¯ÛŒØ¯
- `wait`: ØµØ¨Ø± Ú©Ø±Ø¯Ù†

**Shell Agent:**
- `upload_shell`: Ø¢Ù¾Ù„ÙˆØ¯ shell
- `exploit_vulnerability`: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¢Ø³ÛŒØ¨â€ŒÙ¾Ø°ÛŒØ±ÛŒ
- `escalate_privileges`: Ø§ÙØ²Ø§ÛŒØ´ Ø³Ø·Ø­ Ø¯Ø³ØªØ±Ø³ÛŒ
- `establish_persistence`: Ø§ÛŒØ¬Ø§Ø¯ Ù¾Ø§ÛŒØ¯Ø§Ø±ÛŒ

### 3. Reward (Ù¾Ø§Ø¯Ø§Ø´)

Reward Ù†Ø´Ø§Ù†â€ŒØ¯Ù‡Ù†Ø¯Ù‡ Ù…ÙˆÙÙ‚ÛŒØª ÛŒØ§ Ø´Ú©Ø³Øª action Ø§Ø³Øª:

```python
R = w1*success + w2*speed + w3*stealth + w4*damage - w5*detection

# Ù…Ø«Ø§Ù„:
R = 10.0*(1) + 2.0*(0.8) + 5.0*(0.9) + 3.0*(0.7) - 10.0*(0) = 19.2
```

**Ù…Ø¤Ù„ÙÙ‡â€ŒÙ‡Ø§ÛŒ Reward:**

| Ù…Ø¤Ù„ÙÙ‡ | ÙˆØ²Ù† Ù¾ÛŒØ´â€ŒÙØ±Ø¶ | ØªÙˆØ¶ÛŒØ­ |
|------|-------------|--------|
| `success` | 10.0 | Ø¢ÛŒØ§ Ø­Ù…Ù„Ù‡ Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯ØŸ (0 ÛŒØ§ 1) |
| `speed` | 2.0 | Ø³Ø±Ø¹Øª Ø­Ù…Ù„Ù‡ (0-1) |
| `stealth` | 5.0 | Ù…ÛŒØ²Ø§Ù† Ù…Ø®ÙÛŒ Ù…Ø§Ù†Ø¯Ù† (0-1) |
| `damage` | 3.0 | Ù…ÛŒØ²Ø§Ù† Ø¢Ø³ÛŒØ¨ ÙˆØ§Ø±Ø¯ Ø´Ø¯Ù‡ (0-1) |
| `detection` | -10.0 | Ø¬Ø±ÛŒÙ…Ù‡ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯Ù† |

### 4. Episode (Ø¯ÙˆØ±Ù‡)

Episode ÛŒÚ© Ø¯Ù†Ø¨Ø§Ù„Ù‡ Ú©Ø§Ù…Ù„ Ø§Ø² ØªØ¹Ø§Ù…Ù„Ø§Øª Ø§Ø³Øª:

```
Episode = (sâ‚€, aâ‚€, râ‚€) â†’ (sâ‚, aâ‚, râ‚) â†’ ... â†’ (sâ‚™, aâ‚™, râ‚™) â†’ Terminal State
```

Ù…Ø«Ø§Ù„:
```python
episode_id = rl_engine.start_episode(RLAgentType.DDOS, initial_state)

for step in range(max_steps):
    action = rl_engine.select_action(RLAgentType.DDOS, current_state)
    next_state, reward, done = environment.step(action)
    rl_engine.store_experience(RLAgentType.DDOS, current_state, action, 
                               reward, next_state, done)
    
    if done:
        break
    
    current_state = next_state

rl_engine.end_episode(RLAgentType.DDOS, success=True, total_reward=27.5, metrics={...})
```

---

## ğŸš€ Ù†Ø­ÙˆÙ‡ Ø§Ø³ØªÙØ§Ø¯Ù‡

### Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø§ÙˆÙ„ÛŒÙ‡

```python
from core.rl_engine import get_rl_engine, RLAgentType, RLState, RLAction

# Ø¯Ø±ÛŒØ§ÙØª instance Ù…ÙˆØªÙˆØ± RL
rl_engine = get_rl_engine()
```

### Ø³Ù†Ø§Ø±ÛŒÙˆÛŒ Ú©Ø§Ù…Ù„: Ø­Ù…Ù„Ù‡ DDoS

```python
# 1. ØªØ¹Ø±ÛŒÙ ÙˆØ¶Ø¹ÛŒØª Ø§ÙˆÙ„ÛŒÙ‡
initial_state = RLState(
    target_ip="192.168.1.100",
    target_ports=[80, 443],
    target_os="Linux",
    target_services={"http": "nginx", "https": "nginx"},
    network_latency=50.0,
    bandwidth=1000.0,
    firewall_active=True,
    ids_active=True,
    attack_stage=0,
    time_elapsed=0.0,
    packets_sent=0,
    success_rate=0.0,
    previous_actions=[],
    detection_count=0
)

# 2. Ø´Ø±ÙˆØ¹ Episode
episode_id = rl_engine.start_episode(
    agent_type=RLAgentType.DDOS,
    initial_state=initial_state,
    context={'researcher_id': 'RES-001', 'experiment_name': 'DDoS-Test-1'}
)

print(f"Episode Ø´Ø±ÙˆØ¹ Ø´Ø¯: {episode_id}")

# 3. Ø­Ù„Ù‚Ù‡ Ø§ØµÙ„ÛŒ Ø­Ù…Ù„Ù‡
total_reward = 0.0
current_state = initial_state

for step in range(10):  # Ø­Ø¯Ø§Ú©Ø«Ø± 10 Ú¯Ø§Ù…
    # Ø§Ù†ØªØ®Ø§Ø¨ action Ø¨Ù‡ÛŒÙ†Ù‡
    action_idx = rl_engine.select_action(
        agent_type=RLAgentType.DDOS,
        state=current_state,
        explore=True  # exploration Ø¯Ø± Ù…Ø±Ø§Ø­Ù„ Ø§ÙˆÙ„ÛŒÙ‡
    )
    
    # ØªØ¨Ø¯ÛŒÙ„ Ø§ÛŒÙ†Ø¯Ú©Ø³ Ø¨Ù‡ action ÙˆØ§Ù‚Ø¹ÛŒ
    action = RLAction(
        action_type=f"action_{action_idx}",
        parameters={'intensity': 0.5 + (action_idx * 0.1)}
    )
    
    # Ø§Ø¹Ù…Ø§Ù„ action Ø¯Ø± Ù…Ø­ÛŒØ· (Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ)
    # Ø¯Ø± ÙˆØ§Ù‚Ø¹ÛŒØªØŒ Ø§ÛŒÙ† Ú©Ø¯ simulation module Ø±Ø§ ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
    next_state, reward, done = simulate_ddos_step(current_state, action)
    
    total_reward += reward
    
    # Ø°Ø®ÛŒØ±Ù‡ ØªØ¬Ø±Ø¨Ù‡
    rl_engine.store_experience(
        agent_type=RLAgentType.DDOS,
        state=current_state,
        action=action,
        reward=reward,
        next_state=next_state,
        done=done,
        priority=abs(reward)  # ØªØ¬Ø±Ø¨ÛŒØ§Øª Ø¨Ø§ reward Ø¨Ø§Ù„Ø§ØŒ priority Ø¨ÛŒØ´ØªØ±
    )
    
    print(f"Step {step+1}: Action={action_idx}, Reward={reward:.2f}, Total={total_reward:.2f}")
    
    if done:
        print("Ø­Ù…Ù„Ù‡ Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯ (Ù…ÙˆÙÙ‚ ÛŒØ§ Ø´Ú©Ø³Øª)")
        break
    
    current_state = next_state

# 4. Ù¾Ø§ÛŒØ§Ù† Episode
rl_engine.end_episode(
    agent_type=RLAgentType.DDOS,
    success=(total_reward > 0),
    total_reward=total_reward,
    metrics={
        'success_rate': 1.0 if total_reward > 0 else 0.0,
        'average_reward': total_reward / max(step + 1, 1),
        'total_damage': 0.8,
        'stealth_score': 0.6
    }
)

print(f"\nâœ“ Episode Ù¾Ø§ÛŒØ§Ù† ÛŒØ§ÙØª - Total Reward: {total_reward:.2f}")

# 5. Ø¨Ø±Ø±Ø³ÛŒ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¨Ø§Ø²Ø¢Ù…ÙˆØ²ÛŒ
if rl_engine.should_retrain(RLAgentType.DDOS):
    print("\nâš ï¸  Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¨Ø§Ø²Ø¢Ù…ÙˆØ²ÛŒ - Ø´Ø±ÙˆØ¹ training...")
    rl_engine.train_agent(
        agent_type=RLAgentType.DDOS,
        batch_size=64,
        epochs=10
    )
    print("âœ“ Ø¨Ø§Ø²Ø¢Ù…ÙˆØ²ÛŒ ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯")

# 6. Ø¯Ø±ÛŒØ§ÙØª Ø¢Ù…Ø§Ø±
stats = rl_engine.get_statistics(RLAgentType.DDOS)
print(f"\nØ¢Ù…Ø§Ø± Agent:")
for key, value in stats.items():
    print(f"  {key}: {value}")
```

### ØªØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ: Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ (Ù…Ø«Ø§Ù„)

```python
def simulate_ddos_step(state: RLState, action: RLAction) -> Tuple[RLState, float, bool]:
    """
    Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ ÛŒÚ© Ú¯Ø§Ù… Ø­Ù…Ù„Ù‡ DDoS
    
    Returns:
        next_state: ÙˆØ¶Ø¹ÛŒØª Ø¨Ø¹Ø¯ÛŒ
        reward: Ù¾Ø§Ø¯Ø§Ø´
        done: Ø¢ÛŒØ§ Ø­Ù…Ù„Ù‡ ØªÙ…Ø§Ù… Ø´Ø¯ØŸ
    """
    # Ù…Ø­Ø§Ø³Ø¨Ù‡ reward Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² RewardFunction
    from core.rl_engine import RewardFunction
    
    reward_func = RewardFunction()
    
    # Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ù†ØªÛŒØ¬Ù‡ action
    success = (action.parameters.get('intensity', 0.5) > 0.7)
    time_taken = 10.0  # Ø«Ø§Ù†ÛŒÙ‡
    stealth_score = 1.0 - action.parameters.get('intensity', 0.5)
    damage_level = action.parameters.get('intensity', 0.5)
    detected = (action.parameters.get('intensity', 0.5) > 0.9)
    
    reward = reward_func.calculate(
        success=success,
        time_taken=time_taken,
        stealth_score=stealth_score,
        damage_level=damage_level,
        detected=detected
    )
    
    # Ø§ÛŒØ¬Ø§Ø¯ next_state
    next_state = RLState(
        target_ip=state.target_ip,
        target_ports=state.target_ports,
        target_os=state.target_os,
        target_services=state.target_services,
        network_latency=state.network_latency,
        bandwidth=state.bandwidth,
        firewall_active=state.firewall_active,
        ids_active=state.ids_active,
        attack_stage=state.attack_stage + 1,
        time_elapsed=state.time_elapsed + time_taken,
        packets_sent=state.packets_sent + 1000,
        success_rate=0.5 if success else 0.0,
        previous_actions=state.previous_actions + [action.action_type],
        detection_count=state.detection_count + (1 if detected else 0)
    )
    
    # Ø­Ù…Ù„Ù‡ ØªÙ…Ø§Ù… Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ø§Ú¯Ø±:
    # - Ù…ÙˆÙÙ‚ Ø´Ø¯
    # - Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯
    # - Ø²Ù…Ø§Ù† Ø²ÛŒØ§Ø¯ÛŒ Ú¯Ø°Ø´ØªÙ‡
    done = success or detected or (state.attack_stage >= 10)
    
    return next_state, reward, done
```

---

## âš™ï¸ Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ

### ÙØ§ÛŒÙ„ Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ: `config/config.dev.yaml`

```yaml
rl_engine:
  # Ø§Ø¨Ø¹Ø§Ø¯ ÙØ¶Ø§ÛŒ State Ùˆ Action
  state_dimension: 13          # ØªØ¹Ø¯Ø§Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ State
  action_dimension: 10         # ØªØ¹Ø¯Ø§Ø¯ ActionÙ‡Ø§ÛŒ Ù…Ù…Ú©Ù†
  
  # Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ
  learning_rate: 0.1           # Ù†Ø±Ø® ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ (Î±)
  discount_factor: 0.99        # Ø¶Ø±ÛŒØ¨ ØªØ®ÙÛŒÙ (Î³)
  
  # Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Exploration
  epsilon_start: 1.0           # Ù…Ù‚Ø¯Ø§Ø± Ø§ÙˆÙ„ÛŒÙ‡ Îµ
  epsilon_decay: 0.995         # Ù†Ø±Ø® Ú©Ø§Ù‡Ø´ Îµ
  epsilon_min: 0.01            # Ø­Ø¯Ø§Ù‚Ù„ Îµ
  
  # Experience Replay
  replay_buffer_size: 100000   # Ø¸Ø±ÙÛŒØª buffer
  priority_alpha: 0.6          # Ù…ÛŒØ²Ø§Ù† Ø§ÙˆÙ„ÙˆÛŒØªâ€ŒØ¯Ù‡ÛŒ
  priority_beta: 0.4           # Ù…ÛŒØ²Ø§Ù† importance sampling
  
  # Ø¨Ø§Ø²Ø¢Ù…ÙˆØ²ÛŒ
  retrain_interval: 100        # Ø¨Ø§Ø²Ø¢Ù…ÙˆØ²ÛŒ Ù‡Ø± N episode
  batch_size: 64               # ØªØ¹Ø¯Ø§Ø¯ sample Ø¯Ø± Ù‡Ø± batch
  training_epochs: 10          # ØªØ¹Ø¯Ø§Ø¯ epoch Ø¯Ø± Ù‡Ø± Ø¨Ø§Ø²Ø¢Ù…ÙˆØ²ÛŒ
  
  # Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ
  save_episodes_to_db: true    # Ø°Ø®ÛŒØ±Ù‡ EpisodeÙ‡Ø§ Ø¯Ø± DB
  save_models_to_db: true      # Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¯Ø± DB
  model_save_interval: 50      # Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„ Ù‡Ø± N episode
  
  # ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ Reward Function
  reward_success: 10.0         # ÙˆØ²Ù† Ù…ÙˆÙÙ‚ÛŒØª
  reward_speed: 2.0            # ÙˆØ²Ù† Ø³Ø±Ø¹Øª
  reward_stealth: 5.0          # ÙˆØ²Ù† Ù…Ø®ÙÛŒ Ù…Ø§Ù†Ø¯Ù†
  reward_damage: 3.0           # ÙˆØ²Ù† Ø¢Ø³ÛŒØ¨
  reward_detection_penalty: -10.0  # Ø¬Ø±ÛŒÙ…Ù‡ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ
  
  # Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø´Ø¯Ù‡
  algorithms:
    - q_learning              # Q-Learning Ú©Ù„Ø§Ø³ÛŒÚ©
    - deep_q_network          # DQN
    - policy_gradient         # REINFORCE
    - actor_critic            # A2C/A3C
    - ppo                     # Proximal Policy Optimization
```

### ØªØºÛŒÛŒØ± Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ø¯Ø± Runtime

```python
from core.config_manager import get_config

config = get_config()

# Ø®ÙˆØ§Ù†Ø¯Ù† Ù…Ù‚Ø¯Ø§Ø±
learning_rate = config.get('rl_engine.learning_rate', 0.1)

# ØªØºÛŒÛŒØ± Ù…ÙˆÙ‚Øª (ÙÙ‚Ø· Ø¯Ø± Ø­Ø§ÙØ¸Ù‡)
config.set('rl_engine.epsilon_start', 0.5)

# Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¦Ù…ÛŒ
config.save()
```

---

## ğŸ’¾ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡

### Schema Overview

RL Engine Ø§Ø² 4 Ø¬Ø¯ÙˆÙ„ Ø§ØµÙ„ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯:

#### 1. `rl_experiences` - ØªØ¬Ø±Ø¨ÛŒØ§Øª RL

```sql
CREATE TABLE rl_experiences (
    id SERIAL PRIMARY KEY,
    agent_type VARCHAR(50) NOT NULL,
    episode_id UUID NOT NULL,
    step_number INTEGER NOT NULL,
    state_json JSONB NOT NULL,
    action_json JSONB NOT NULL,
    reward FLOAT NOT NULL,
    next_state_json JSONB NOT NULL,
    done BOOLEAN DEFAULT FALSE,
    timestamp TIMESTAMP DEFAULT NOW(),
    priority FLOAT DEFAULT 1.0
);
```

**Ø§Ø³ØªÙØ§Ø¯Ù‡:**
- Ø°Ø®ÛŒØ±Ù‡ ØªÙ…Ø§Ù… ØªØ¬Ø±Ø¨ÛŒØ§Øª Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø²Ø¢Ù…ÙˆØ²ÛŒ
- Sample Ú©Ø±Ø¯Ù† Ø¨Ø§ priority Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´
- ØªØ­Ù„ÛŒÙ„ Ø±ÙØªØ§Ø± Agent Ø¯Ø± Ú¯Ø°Ø´ØªÙ‡

**Ù…Ø«Ø§Ù„ Query:**
```sql
-- ØªØ¬Ø±Ø¨ÛŒØ§Øª Ø¨Ø§ Ø¨Ø§Ù„Ø§ØªØ±ÛŒÙ† priority
SELECT * FROM rl_experiences
WHERE agent_type = 'ddos'
ORDER BY priority DESC
LIMIT 100;
```

#### 2. `rl_episodes` - Ù†ØªØ§ÛŒØ¬ Episode

```sql
CREATE TABLE rl_episodes (
    id UUID PRIMARY KEY,
    agent_type VARCHAR(50) NOT NULL,
    start_time TIMESTAMP NOT NULL,
    end_time TIMESTAMP,
    total_reward FLOAT,
    steps_count INTEGER,
    success BOOLEAN,
    success_rate FLOAT,
    average_reward FLOAT,
    total_damage FLOAT,
    stealth_score FLOAT,
    model_version INTEGER
);
```

**Ø§Ø³ØªÙØ§Ø¯Ù‡:**
- Ø±Ø¯ÛŒØ§Ø¨ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Agent Ø¯Ø± Ø·ÙˆÙ„ Ø²Ù…Ø§Ù†
- Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù†Ø³Ø®Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ù…Ø¯Ù„
- ØªØ­Ù„ÛŒÙ„ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù…ÙˆÙÙ‚ÛŒØª

**Ù…Ø«Ø§Ù„ Query:**
```sql
-- 10 Ø¨Ù‡ØªØ±ÛŒÙ† Episode
SELECT * FROM rl_episodes
WHERE agent_type = 'ddos' AND success = true
ORDER BY total_reward DESC
LIMIT 10;
```

#### 3. `rl_models` - Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´â€ŒØ¯ÛŒØ¯Ù‡

```sql
CREATE TABLE rl_models (
    id SERIAL PRIMARY KEY,
    agent_type VARCHAR(50) NOT NULL,
    version INTEGER NOT NULL,
    model_weights BYTEA NOT NULL,
    training_episodes INTEGER,
    average_reward FLOAT,
    success_rate FLOAT,
    trained_at TIMESTAMP DEFAULT NOW(),
    is_active BOOLEAN DEFAULT FALSE
);
```

**Ø§Ø³ØªÙØ§Ø¯Ù‡:**
- Ù†Ø³Ø®Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§
- A/B Testing
- Rollback Ø¯Ø± ØµÙˆØ±Øª Ú©Ø§Ù‡Ø´ Ø¹Ù…Ù„Ú©Ø±Ø¯

**Ù…Ø«Ø§Ù„ Query:**
```sql
-- Ù…Ø¯Ù„ ÙØ¹Ø§Ù„ ÙØ¹Ù„ÛŒ
SELECT * FROM rl_models
WHERE agent_type = 'ddos' AND is_active = true;
```

#### 4. `rl_agent_stats` - Ø¢Ù…Ø§Ø± Ø±ÙˆØ²Ø§Ù†Ù‡

```sql
CREATE TABLE rl_agent_stats (
    id SERIAL PRIMARY KEY,
    agent_type VARCHAR(50) NOT NULL,
    date DATE NOT NULL DEFAULT CURRENT_DATE,
    total_episodes INTEGER DEFAULT 0,
    successful_episodes INTEGER DEFAULT 0,
    failed_episodes INTEGER DEFAULT 0,
    total_reward FLOAT DEFAULT 0.0,
    average_reward FLOAT DEFAULT 0.0,
    epsilon FLOAT DEFAULT 1.0,
    training_steps INTEGER DEFAULT 0
);
```

**Ø§Ø³ØªÙØ§Ø¯Ù‡:**
- Ù†Ù…Ø§ÛŒØ´ Ù¾ÛŒØ´Ø±ÙØª ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ
- Dashboard Ø¢Ù…Ø§Ø±ÛŒ
- ØªØ´Ø®ÛŒØµ Ù…Ø´Ú©Ù„Ø§Øª Ø¹Ù…Ù„Ú©Ø±Ø¯

---

## ğŸ§® Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§

### 1. Q-Learning (Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯Ù‡)

**ÙØ±Ù…ÙˆÙ„:**
```
Q(s,a) â† Q(s,a) + Î±[r + Î³ max Q(s',a') - Q(s,a)]
```

**Ù…Ø²Ø§ÛŒØ§:**
- âœ… Ø³Ø§Ø¯Ù‡ Ùˆ Ù‚Ø§Ø¨Ù„ ÙÙ‡Ù…
- âœ… Ø¨Ø¯ÙˆÙ† Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ
- âœ… Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ ÙØ¶Ø§Ù‡Ø§ÛŒ Ú©ÙˆÚ†Ú©

**Ù…Ø¹Ø§ÛŒØ¨:**
- âŒ Ø¨Ø±Ø§ÛŒ ÙØ¶Ø§Ù‡Ø§ÛŒ Ø¨Ø²Ø±Ú¯ Ù…Ù‚ÛŒØ§Ø³â€ŒÙ¾Ø°ÛŒØ± Ù†ÛŒØ³Øª
- âŒ Ù†Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ State Ù‡Ø§ÛŒ Ù¾ÛŒÙˆØ³ØªÙ‡ Ø±Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ú©Ù†Ø¯

### 2. Deep Q-Network (DQN) - Ø¯Ø± Ø­Ø§Ù„ ØªÙˆØ³Ø¹Ù‡

**Ù…Ø¹Ù…Ø§Ø±ÛŒ:**
```
State Vector (13) â†’ Dense(64) â†’ ReLU â†’ Dense(32) â†’ ReLU â†’ Dense(10) â†’ Q-Values
```

**Ù…Ø²Ø§ÛŒØ§:**
- âœ… Ù…Ù‚ÛŒØ§Ø³â€ŒÙ¾Ø°ÛŒØ± Ø¨Ù‡ ÙØ¶Ø§Ù‡Ø§ÛŒ Ø¨Ø²Ø±Ú¯
- âœ… Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø±Ø§ Ø®ÙˆØ¯Ú©Ø§Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù†Ø¯
- âœ… Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¨Ù‡ØªØ± Ø¯Ø± Ù…Ø³Ø§Ø¦Ù„ Ù¾ÛŒÚ†ÛŒØ¯Ù‡

### 3. Policy Gradient - Ø¯Ø± Ø­Ø§Ù„ ØªÙˆØ³Ø¹Ù‡

**ÙØ±Ù…ÙˆÙ„:**
```
âˆ‡J(Î¸) = E[âˆ‡log Ï€(a|s) * R]
```

**Ù…Ø²Ø§ÛŒØ§:**
- âœ… Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ ActionÙ‡Ø§ÛŒ Ù¾ÛŒÙˆØ³ØªÙ‡
- âœ… Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø³ÛŒØ§Ø³Øªâ€ŒÙ‡Ø§ÛŒ ØªØµØ§Ø¯ÙÛŒ ÛŒØ§Ø¯ Ø¨Ú¯ÛŒØ±Ø¯

### 4. Proximal Policy Optimization (PPO) - Ø¯Ø± Ø­Ø§Ù„ ØªÙˆØ³Ø¹Ù‡

**Ø¨Ù‡ØªØ±ÛŒÙ† Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ø¨Ø±Ø§ÛŒ SecureRedLab**

**Ù…Ø²Ø§ÛŒØ§:**
- âœ… Ù¾Ø§ÛŒØ¯Ø§Ø± Ùˆ Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªÙ…Ø§Ø¯
- âœ… Ù†ÛŒØ§Ø² Ø¨Ù‡ Ú©Ù…ØªØ±ÛŒÙ† ØªÙ†Ø¸ÛŒÙ… Ù¾Ø§Ø±Ø§Ù…ØªØ±
- âœ… Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¹Ø§Ù„ÛŒ Ø¯Ø± Ù…Ø³Ø§Ø¦Ù„ Ù¾ÛŒÚ†ÛŒØ¯Ù‡

---

## ğŸ¯ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ

### 1. Priority Experience Replay

Ø¨Ù‡ Ø¬Ø§ÛŒ Sample Ú©Ø±Ø¯Ù† ØªØµØ§Ø¯ÙÛŒØŒ ØªØ¬Ø±Ø¨ÛŒØ§Øª Ù…Ù‡Ù…â€ŒØªØ± Ø¨ÛŒØ´ØªØ± sample Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯:

```python
# Ø§ÙˆÙ„ÙˆÛŒØª = |TD Error|
priority = abs(reward + gamma * max_q_next - q_current)

# Sample Ø¨Ø§ Ø§Ø­ØªÙ…Ø§Ù„
P(i) = priority_i^Î± / Î£ priority_j^Î±
```

**ØªØ£Ø«ÛŒØ±:** ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ 2-3 Ø¨Ø±Ø§Ø¨Ø± Ø³Ø±ÛŒØ¹â€ŒØªØ±

### 2. Importance Sampling

Ø¨Ø±Ø§ÛŒ Ø¬Ø¨Ø±Ø§Ù† bias Ù†Ø§Ø´ÛŒ Ø§Ø² Priority Sampling:

```python
# ÙˆØ²Ù† importance sampling
w_i = (N * P(i))^(-Î²)
w_i = w_i / max(w)  # Normalize
```

### 3. Îµ-Greedy Decay

Ú©Ø§Ù‡Ø´ ØªØ¯Ø±ÛŒØ¬ÛŒ Exploration:

```python
# Epoch 0:   Îµ = 1.0   (100% exploration)
# Epoch 100: Îµ = 0.606 (60% exploration)
# Epoch 500: Îµ = 0.082 (8% exploration)
# Epoch 1000: Îµ = 0.01 (1% exploration - minimum)
```

### 4. Batch Training

Ø¢Ù…ÙˆØ²Ø´ Ø¨Ø§ batch Ø¨Ø±Ø§ÛŒ Ø³Ø±Ø¹Øª Ø¨ÛŒØ´ØªØ±:

```python
# Ø¨Ù‡ Ø¬Ø§ÛŒ update ØªÚ©â€ŒØªÚ©
for exp in experiences:
    agent.update(exp)

# Batch update
batch = sample(experiences, size=64)
agent.batch_update(batch)
```

**ØªØ£Ø«ÛŒØ±:** Ø³Ø±Ø¹Øª Ø¢Ù…ÙˆØ²Ø´ 10-20 Ø¨Ø±Ø§Ø¨Ø± Ø¨ÛŒØ´ØªØ±

---

## ğŸ’¡ Ù…Ø«Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø±Ø¯ÛŒ

### Ù…Ø«Ø§Ù„ 1: Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø­Ù…Ù„Ù‡ DDoS

**Ø³Ù†Ø§Ø±ÛŒÙˆ:** ÛŒØ§ÙØªÙ† Ø¨Ù‡ØªØ±ÛŒÙ† ØªØ±Ú©ÛŒØ¨ ØªØ¹Ø¯Ø§Ø¯ bot Ùˆ Ù‚Ø¯Ø±Øª Ø­Ù…Ù„Ù‡

```python
# ØªØ¹Ø±ÛŒÙ ÙØ¶Ø§ÛŒ Action
actions = [
    {'bots': 1000, 'power': 0.1},
    {'bots': 5000, 'power': 0.3},
    {'bots': 10000, 'power': 0.5},
    {'bots': 50000, 'power': 0.7},
    {'bots': 100000, 'power': 0.9}
]

# Ø¨Ø¹Ø¯ Ø§Ø² 1000 Episode:
# Agent ÛŒØ§Ø¯ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯ Ú©Ù‡:
# - Ø¨Ø±Ø§ÛŒ Ø§Ù‡Ø¯Ø§Ù Ø¨Ø§ ÙØ§ÛŒØ±ÙˆØ§Ù„ Ø¶Ø¹ÛŒÙ: Ø¨Ù‡ØªØ±ÛŒÙ† = {bots: 5000, power: 0.3}
# - Ø¨Ø±Ø§ÛŒ Ø§Ù‡Ø¯Ø§Ù Ø¨Ø§ IDS Ù‚ÙˆÛŒ: Ø¨Ù‡ØªØ±ÛŒÙ† = {bots: 100000, power: 0.9}
```

### Ù…Ø«Ø§Ù„ 2: Shell Upload Ù‡ÙˆØ´Ù…Ù†Ø¯

**Ø³Ù†Ø§Ø±ÛŒÙˆ:** Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ù‡ØªØ±ÛŒÙ† Ø±ÙˆØ´ Ø¢Ù¾Ù„ÙˆØ¯ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø³ÛŒØ³ØªÙ…â€ŒØ¹Ø§Ù…Ù„

```python
# Ø¨Ø¹Ø¯ Ø§Ø² 500 Episode:
# Agent ÛŒØ§Ø¯ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯ Ú©Ù‡:
# - Windows + IIS â†’ Ø±ÙˆØ´: File upload vulnerability
# - Linux + Apache â†’ Ø±ÙˆØ´: Remote code execution
# - Ø¨Ø§ WAF â†’ Ø±ÙˆØ´: Obfuscated payload
```

### Ù…Ø«Ø§Ù„ 3: Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ù¾Ø³ Ø§Ø² Ø´Ú©Ø³Øª

**Ø³Ù†Ø§Ø±ÛŒÙˆ:** Agent ÛŒØ§Ø¯ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯ Ø§Ø² Ø´Ú©Ø³Øªâ€ŒÙ‡Ø§ÛŒ Ù‚Ø¨Ù„ÛŒ Ø¯Ø±Ø³ Ø¨Ú¯ÛŒØ±Ø¯

```python
# Episode 1-100: ØªÙ„Ø§Ø´ Ù…Ø³ØªÙ‚ÛŒÙ… â†’ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯Ù‡ â†’ Ø´Ú©Ø³Øª
# Episode 101-200: Agent Ù…ÛŒâ€ŒØ¢Ù…ÙˆØ²Ø¯ Ø§Ø¨ØªØ¯Ø§ reconnaissance Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡Ø¯
# Episode 201-300: Agent Ù…ÛŒâ€ŒØ¢Ù…ÙˆØ²Ø¯ Ø¨Ø§ Ø³Ø±Ø¹Øª Ú©Ù… Ø´Ø±ÙˆØ¹ Ú©Ù†Ø¯
# Episode 301+: Ù†Ø±Ø® Ù…ÙˆÙÙ‚ÛŒØª Ø§Ø² 10% Ø¨Ù‡ 80% Ù…ÛŒâ€ŒØ±Ø³Ø¯
```

---

## ğŸ› Ø¹ÛŒØ¨â€ŒÛŒØ§Ø¨ÛŒ

### Ù…Ø´Ú©Ù„ 1: Agent ÛŒØ§Ø¯ Ù†Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯

**Ø¹Ù„Ø§Ø¦Ù…:**
- Reward Ø¨Ù‡ ØµÙˆØ±Øª Ø«Ø§Ø¨Øª Ù…ÛŒâ€ŒÙ…Ø§Ù†Ø¯
- Success rate Ø¨Ù‡Ø¨ÙˆØ¯ Ù¾ÛŒØ¯Ø§ Ù†Ù…ÛŒâ€ŒÚ©Ù†Ø¯

**Ø±Ø§Ù‡â€ŒØ­Ù„:**
```python
# 1. Ø¨Ø±Ø±Ø³ÛŒ Îµ (Ø¨Ø§ÛŒØ¯ Ø¨Ù‡ ØªØ¯Ø±ÛŒØ¬ Ú©Ø§Ù‡Ø´ ÛŒØ§Ø¨Ø¯)
stats = rl_engine.get_statistics(RLAgentType.DDOS)
print(f"Epsilon: {stats['epsilon']}")  # Ø¨Ø§ÛŒØ¯ < 0.5 Ø¨Ø§Ø´Ø¯ Ø¨Ø¹Ø¯ Ø§Ø² 200 episode

# 2. Ø§ÙØ²Ø§ÛŒØ´ learning rate
config.set('rl_engine.learning_rate', 0.3)

# 3. Ø¨Ø±Ø±Ø³ÛŒ Reward Function
# Ø¢ÛŒØ§ rewardÙ‡Ø§ Ù…Ø¹Ù†Ø§Ø¯Ø§Ø± Ù‡Ø³ØªÙ†Ø¯ØŸ
```

### Ù…Ø´Ú©Ù„ 2: Agent Ø¨ÛŒØ´â€ŒØ¨Ø±Ø§Ø²Ø´ Ù…ÛŒâ€ŒÚ©Ù†Ø¯

**Ø¹Ù„Ø§Ø¦Ù…:**
- Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ³Øª Ø¶Ø¹ÛŒÙ Ø§Ø³Øª
- Agent ÙÙ‚Ø· Ø¯Ø± Ù…ÙˆÙ‚Ø¹ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø®Ø§Øµ Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯

**Ø±Ø§Ù‡â€ŒØ­Ù„:**
```python
# 1. Ø§ÙØ²Ø§ÛŒØ´ Exploration
config.set('rl_engine.epsilon_min', 0.1)  # Ø¨Ù‡ Ø¬Ø§ÛŒ 0.01

# 2. Ø§ÙØ²Ø§ÛŒØ´ ØªÙ†ÙˆØ¹ Ø¯Ø§Ø¯Ù‡
# Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø§Ù‡Ø¯Ø§Ù Ù…Ø®ØªÙ„Ù Ø¯Ø± Ø¢Ù…ÙˆØ²Ø´

# 3. Early stopping
# Ù…ØªÙˆÙ‚Ù Ú©Ø±Ø¯Ù† Ø¢Ù…ÙˆØ²Ø´ Ø²Ù…Ø§Ù†ÛŒ Ú©Ù‡ validation error Ø§ÙØ²Ø§ÛŒØ´ ÛŒØ§ÙØª
```

### Ù…Ø´Ú©Ù„ 3: Buffer Ù¾Ø± Ø§Ø³Øª

**Ø¹Ù„Ø§Ø¦Ù…:**
```
WARNING: Experience Replay Buffer is full (100000/100000)
```

**Ø±Ø§Ù‡â€ŒØ­Ù„:**
```python
# 1. Ø§ÙØ²Ø§ÛŒØ´ Ø¸Ø±ÙÛŒØª
config.set('rl_engine.replay_buffer_size', 200000)

# 2. Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ùˆ Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† buffer
buffer = rl_engine.replay_buffers[RLAgentType.DDOS]
buffer.save_to_database(db_manager)
buffer.buffer.clear()

# 3. Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Prioritized Eviction
# ØªØ¬Ø±Ø¨ÛŒØ§Øª Ø¨Ø§ priority Ù¾Ø§ÛŒÛŒÙ† Ø­Ø°Ù Ø´ÙˆÙ†Ø¯
```

### Ù…Ø´Ú©Ù„ 4: Ø¢Ù…ÙˆØ²Ø´ Ø¨Ø³ÛŒØ§Ø± Ú©Ù†Ø¯ Ø§Ø³Øª

**Ø±Ø§Ù‡â€ŒØ­Ù„:**
```python
# 1. Ú©Ø§Ù‡Ø´ batch size
config.set('rl_engine.batch_size', 32)  # Ø¨Ù‡ Ø¬Ø§ÛŒ 64

# 2. Ú©Ø§Ù‡Ø´ frequency Ø¨Ø§Ø²Ø¢Ù…ÙˆØ²ÛŒ
config.set('rl_engine.retrain_interval', 200)  # Ø¨Ù‡ Ø¬Ø§ÛŒ 100

# 3. Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² GPU (Ø§Ú¯Ø± DQN Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒØ¯)
# Ø¯Ø± Ø¢ÛŒÙ†Ø¯Ù‡ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø®ÙˆØ§Ù‡Ø¯ Ø´Ø¯
```

---

## ğŸ“Š Ù†Ø¸Ø§Ø±Øª Ùˆ Monitoring

### Dashboard Ø¢Ù…Ø§Ø±ÛŒ

```python
# Ø¯Ø±ÛŒØ§ÙØª Ø¢Ù…Ø§Ø± ØªÙ…Ø§Ù… AgentÙ‡Ø§
for agent_type in RLAgentType:
    stats = rl_engine.get_statistics(agent_type)
    print(f"\n{agent_type.value}:")
    print(f"  Episodes: {stats['total_episodes']}")
    print(f"  Avg Reward: {stats['average_reward']:.2f}")
    print(f"  Epsilon: {stats['epsilon']:.3f}")
    print(f"  Buffer Size: {stats['buffer_size']}")
```

### Ù†Ù…ÙˆØ¯Ø§Ø± Ù¾ÛŒØ´Ø±ÙØª

```sql
-- Query Ø¨Ø±Ø§ÛŒ Ù†Ù…ÙˆØ¯Ø§Ø± Ù¾ÛŒØ´Ø±ÙØª
SELECT 
    date,
    agent_type,
    average_reward,
    average_success_rate,
    epsilon
FROM rl_agent_stats
WHERE agent_type = 'ddos'
ORDER BY date;
```

### Ù‡Ø´Ø¯Ø§Ø±Ù‡Ø§ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø±

```python
# Ø¨Ø±Ø±Ø³ÛŒ Ú©Ø§Ù‡Ø´ Ø¹Ù…Ù„Ú©Ø±Ø¯
current_success_rate = get_current_success_rate(agent_type)
if current_success_rate < 0.5:
    logger.warning(f"Ø¹Ù…Ù„Ú©Ø±Ø¯ {agent_type} Ú©Ø§Ù‡Ø´ ÛŒØ§ÙØªÙ‡ - Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¨Ø±Ø±Ø³ÛŒ")
    # Ø§Ø±Ø³Ø§Ù„ Ø§Ø¹Ù„Ø§Ù†
```

---

## ğŸ”¬ ØªØ­Ù‚ÛŒÙ‚ Ùˆ ØªÙˆØ³Ø¹Ù‡

### Ø§ÛŒØ¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢ÛŒÙ†Ø¯Ù‡

1. **Multi-Agent RL**: Ù‡Ù…Ú©Ø§Ø±ÛŒ Ø¨ÛŒÙ† AgentÙ‡Ø§
2. **Meta-Learning**: ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù†Ø­ÙˆÙ‡ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ
3. **Curriculum Learning**: Ø¢Ù…ÙˆØ²Ø´ ØªØ¯Ø±ÛŒØ¬ÛŒ Ø§Ø² Ø³Ø§Ø¯Ù‡ Ø¨Ù‡ Ù¾ÛŒÚ†ÛŒØ¯Ù‡
4. **Transfer Learning**: Ø§Ù†ØªÙ‚Ø§Ù„ Ø¯Ø§Ù†Ø´ Ø¨ÛŒÙ† AgentÙ‡Ø§
5. **Inverse RL**: ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø§Ø² ØªØ®ØµØµâ€ŒÙ‡Ø§ÛŒ Ø§Ù†Ø³Ø§Ù†ÛŒ

---

## ğŸ“– Ù…Ù†Ø§Ø¨Ø¹

### Ù…Ù‚Ø§Ù„Ø§Øª Ø¹Ù„Ù…ÛŒ

1. Mnih et al. (2015) - Human-level control through deep reinforcement learning
2. Schulman et al. (2017) - Proximal Policy Optimization Algorithms
3. Schaul et al. (2016) - Prioritized Experience Replay

### Ú©ØªØ§Ø¨â€ŒÙ‡Ø§

1. Sutton & Barto - Reinforcement Learning: An Introduction
2. Bertsekas - Dynamic Programming and Optimal Control

---

## ğŸ“ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ

Ø¨Ø±Ø§ÛŒ Ø³ÙˆØ§Ù„Ø§Øª Ùˆ Ù…Ø´Ú©Ù„Ø§Øª:

- **Email**: support@secureredlab.edu
- **Documentation**: `/docs/RL_ENGINE_GUIDE.md`
- **Issues**: Ø¯Ø§Ø®Ù„ Ø³ÛŒØ³ØªÙ… Ù„Ø§Ú¯â€ŒÚ¯ÛŒØ±ÛŒ Ø¨Ø§ Ø³Ø·Ø­ ERROR

---

**ØªÙˆØ¬Ù‡:** Ø§ÛŒÙ† Ø³ÛŒØ³ØªÙ… ØªÙ†Ù‡Ø§ Ø¨Ø±Ø§ÛŒ ØªØ­Ù‚ÛŒÙ‚Ø§Øª Ø¢Ú©Ø§Ø¯Ù…ÛŒÚ© Ùˆ Ø¨Ø§ Ù…Ø¬ÙˆØ²Ù‡Ø§ÛŒ Ù‚Ø§Ù†ÙˆÙ†ÛŒ (FBI, IRB, Police, University) Ù‚Ø§Ø¨Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø³Øª.

**Ø§ÛŒÙ† Ø³Ù†Ø¯ Ø¢Ø®Ø±ÛŒÙ† Ø¨Ø§Ø± Ø¯Ø± 2025-01-15 Ø¨Ù‡â€ŒØ±ÙˆØ² Ø´Ø¯.**
