# Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ú©Ø§Ù…Ù„ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ AI Ø¯Ø± SecureRedLab
# Complete AI Models Guide for SecureRedLab

**Ù†Ø³Ø®Ù‡:** 2.0.0  
**ØªØ§Ø±ÛŒØ®:** 2025-12-03  
**Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ­Ù‚ÛŒÙ‚Ø§Øª:** PwnGPT, PentestGPT, Latest Benchmarks

---

## ğŸ“Š **Ø¬Ø¯ÙˆÙ„ Ú©Ø§Ù…Ù„ Ù…Ø¯Ù„â€ŒÙ‡Ø§ (VLM + LLM)**

### **ğŸ–¼ï¸ Vision-Language Models (VLM)**

| Model | Size | Type | API/Local | Cost | Performance | Best For |
|-------|------|------|-----------|------|-------------|----------|
| **Qwen2.5-VL-72B** | 72B | VLM | âœ… Online API | $3-8/1M tokens | â­â­â­â­â­ | Best overall VLM |
| **Gemini 2.0 Flash** | Unknown | VLM | âœ… Online API | $0.075/1M tokens | â­â­â­â­â­ | Fastest, Cheapest |
| **Claude 3.5 Sonnet** | Unknown | VLM | âœ… Online API | $3/1M input | â­â­â­â­â­ | Code analysis |
| **GPT-4 Vision** | Unknown | VLM | âœ… Online API | $10/1M tokens | â­â­â­â­ | General purpose |
| **LLaVA 1.6-34B** | 34B | VLM | ğŸ  **Offline** | Free | â­â­â­â­ | Privacy, Offline |
| **LLaVA 1.6-13B** | 13B | VLM | ğŸ  **Offline** | Free | â­â­â­ | Resource-limited |
| **SmolVLM** | 2B | VLM | ğŸ  **Offline** | Free | â­â­ | Edge devices |

### **ğŸ’¬ Large Language Models (LLM)**

| Model | Size | Type | API/Local | Cost | Performance | Best For |
|-------|------|------|-----------|------|-------------|----------|
| **GLM-4.6** | 6B | LLM | âœ… Online API | $0.3-0.8/1M | â­â­â­â­â­ | **Best value!** |
| **DeepSeek Coder V2** | 236B MoE | LLM | âœ… Online API | $0.14/1M | â­â­â­â­â­ | Code/Exploit gen |
| **Qwen 2.5-72B** | 72B | LLM | âœ… Online + Local | $0.17-0.70/1M | â­â­â­â­â­ | General LLM |
| **Mixtral 8x22B** | 176B MoE | LLM | âœ… Online + Local | $0.9/1M | â­â­â­â­ | Multi-expert |
| **LLaMA 3.1-70B** | 70B | LLM | ğŸ  **Offline** | Free | â­â­â­â­ | Open-source |
| **LLaMA 3.1-8B** | 8B | LLM | ğŸ  **Offline** | Free | â­â­â­ | Resource-limited |

---

## ğŸŒ **Ø¢Ù†Ù„Ø§ÛŒÙ† (Online API) vs ğŸ  Ø¢ÙÙ„Ø§ÛŒÙ† (Offline Local)**

### **âœ… Online API (ØªÙˆØµÛŒÙ‡ Ø¨Ø±Ø§ÛŒ Production)**

**Ù…Ø²Ø§ÛŒØ§:**
- âœ… Ø¨Ø¯ÙˆÙ† Ù†ÛŒØ§Ø² Ø¨Ù‡ GPU
- âœ… Ù‡Ù…ÛŒØ´Ù‡ Ø¢Ø®Ø±ÛŒÙ† Ù†Ø³Ø®Ù‡
- âœ… Ø³Ø±Ø¹Øª Ø¨Ø§Ù„Ø§ (distributed infrastructure)
- âœ… Ù‡Ø²ÛŒÙ†Ù‡ Ù¾Ø§ÛŒÛŒÙ† (pay-per-use)
- âœ… Ù…Ù‚ÛŒØ§Ø³â€ŒÙ¾Ø°ÛŒØ±ÛŒ Ø¨ÛŒâ€ŒÙ†Ù‡Ø§ÛŒØª

**Ù…Ø¹Ø§ÛŒØ¨:**
- âŒ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø§ÛŒÙ†ØªØ±Ù†Øª
- âŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ Ø³Ø±ÙˆØ± Ø§Ø±Ø³Ø§Ù„ Ù…ÛŒâ€ŒØ´ÙˆØ¯ (Privacy concern)
- âŒ Ù…Ø­Ø¯ÙˆØ¯ÛŒØª rate limit

**Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Online:**
```python
# VLM Models
- Qwen2.5-VL-72B: Alibaba Cloud DashScope API
- Gemini 2.0 Flash: Google AI Studio / Vertex AI
- Claude 3.5 Sonnet: Anthropic API
- GPT-4 Vision: OpenAI API

# LLM Models
- GLM-4.6: BigModel API (zhipuai)
- DeepSeek Coder V2: DeepSeek API
- Qwen 2.5-72B: Alibaba Cloud / HuggingFace
- Mixtral 8x22B: Mistral AI API
```

**Ù†Ø­ÙˆÙ‡ Ø¯Ø³ØªØ±Ø³ÛŒ:**
```bash
# Qwen2.5-VL (Alibaba Cloud)
curl https://dashscope.aliyuncs.com/api/v1/services/aigc/multimodal-generation/generation \
  -H "Authorization: Bearer YOUR_API_KEY"

# GLM-4.6 (BigModel)
curl https://open.bigmodel.cn/api/paas/v4/chat/completions \
  -H "Authorization: Bearer YOUR_API_KEY"

# Gemini 2.0 Flash (Google)
curl https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \
  -H "x-goog-api-key: YOUR_API_KEY"
```

---

### **ğŸ  Offline Local (ØªÙˆØµÛŒÙ‡ Ø¨Ø±Ø§ÛŒ Privacy/Research)**

**Ù…Ø²Ø§ÛŒØ§:**
- âœ… Ú©Ù†ØªØ±Ù„ Ú©Ø§Ù…Ù„ Ø¨Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ (Privacy 100%)
- âœ… Ø¨Ø¯ÙˆÙ† Ù…Ø­Ø¯ÙˆØ¯ÛŒØª rate limit
- âœ… Ø¨Ø¯ÙˆÙ† Ù‡Ø²ÛŒÙ†Ù‡ API
- âœ… Ú©Ø§Ø± Ø¨Ø¯ÙˆÙ† Ø§ÛŒÙ†ØªØ±Ù†Øª

**Ù…Ø¹Ø§ÛŒØ¨:**
- âŒ Ù†ÛŒØ§Ø² Ø¨Ù‡ GPU Ù‚ÙˆÛŒ (VRAM Ø¨Ø§Ù„Ø§)
- âŒ Ù‡Ø²ÛŒÙ†Ù‡ Ø³Ø®Øªâ€ŒØ§ÙØ²Ø§Ø± Ø¨Ø§Ù„Ø§
- âŒ Ù†Ú¯Ù‡Ø¯Ø§Ø±ÛŒ Ùˆ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ
- âŒ Ø³Ø±Ø¹Øª Ú©Ù…ØªØ± (Ø¨Ø³ØªÙ‡ Ø¨Ù‡ hardware)

**Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Offline:**
```python
# VLM Models (Offline)
- LLaVA 1.6-34B: 34B params, needs 48GB+ VRAM
- LLaVA 1.6-13B: 13B params, needs 16GB+ VRAM
- SmolVLM: 2B params, needs 4GB+ VRAM

# LLM Models (Offline)
- LLaMA 3.1-70B: needs 80GB+ VRAM (A100)
- LLaMA 3.1-8B: needs 12GB+ VRAM (RTX 3090)
- Qwen 2.5-7B: needs 10GB+ VRAM
```

**Ù†Ø­ÙˆÙ‡ Deploy (Local):**
```bash
# Ø¨Ø§ Ollama (Ø³Ø§Ø¯Ù‡â€ŒØªØ±ÛŒÙ† Ø±Ø§Ù‡)
ollama pull llava:34b-v1.6
ollama run llava:34b-v1.6

# Ø¨Ø§ vLLM (Production)
vllm serve liuhaotian/llava-v1.6-34b \
  --trust-remote-code

# Ø¨Ø§ Hugging Face Transformers
python -c "
from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration
model = LlavaNextForConditionalGeneration.from_pretrained('llava-hf/llava-v1.6-34b-hf')
"
```

---

## ğŸ’° **Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù‡Ø²ÛŒÙ†Ù‡ (Cost Comparison)**

### **Scenario: 1 Million Tokens Processing**

| Model | Type | Online Cost | Offline Cost (VRAM) |
|-------|------|-------------|---------------------|
| **Qwen2.5-VL-72B** | VLM | $3-8 | 80GB VRAM (~$10,000 GPU) |
| **Gemini 2.0 Flash** | VLM | $0.075 | âŒ Not available |
| **Claude 3.5 Sonnet** | VLM | $3 | âŒ Not available |
| **LLaVA 1.6-34B** | VLM | âŒ No API | 48GB VRAM (~$6,000 GPU) |
| **GLM-4.6** | LLM | $0.3-0.8 | âŒ Not available |
| **DeepSeek Coder V2** | LLM | $0.14 | âŒ Not available |
| **LLaMA 3.1-8B** | LLM | Free (via HF) | 12GB VRAM (~$1,000 GPU) |

**ğŸ’¡ Ù†ØªÛŒØ¬Ù‡:**
- **Production â†’ Online API** (Ù‡Ø²ÛŒÙ†Ù‡ Ú©Ù…ØªØ±ØŒ Ù…Ù‚ÛŒØ§Ø³â€ŒÙ¾Ø°ÛŒØ±ÛŒ Ø¨Ù‡ØªØ±)
- **Privacy/Research â†’ Offline Local** (Ú©Ù†ØªØ±Ù„ Ú©Ø§Ù…Ù„ØŒ Ø¨Ø¯ÙˆÙ† Ø§Ø´ØªØ±Ø§Ú© Ø¯Ø§Ø¯Ù‡)

---

## ğŸ¯ **Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ø¨Ø±Ø§ÛŒ SecureRedLab**

### **âœ… Strategy 1: Hybrid (ØªÙˆØµÛŒÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯)**

```python
# Primary: Online APIs (Ø¨Ø±Ø§ÛŒ production)
primary_vlm = "qwen_2_5_vl_72b"      # Alibaba Cloud API
primary_llm = "glm_4_6"               # BigModel API
code_specialist = "deepseek_coder_v2" # DeepSeek API

# Fallback: Offline Models (Ø¨Ø±Ø§ÛŒ privacy/offline scenarios)
fallback_vlm = "llava_1_6_13b"       # Local Ollama
fallback_llm = "llama_3_1_8b"        # Local Ollama
```

**Ú†Ø±Ø§ HybridØŸ**
1. âœ… **99% Ø§Ø³ØªÙØ§Ø¯Ù‡:** Online APIs (Ø³Ø±ÛŒØ¹ØŒ Ø§Ø±Ø²Ø§Ù†ØŒ Ø¨Ø¯ÙˆÙ† GPU)
2. âœ… **1% Ø§Ø³ØªÙØ§Ø¯Ù‡:** Offline (Ø¨Ø±Ø§ÛŒ test Ù‡Ø§ÛŒ Ø­Ø³Ø§Ø³ØŒ Ø¨Ø¯ÙˆÙ† Ø§ÛŒÙ†ØªØ±Ù†Øª)
3. âœ… **Flexibility:** Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ø¨ÛŒÙ† online/offline Ø³ÙˆØ¦ÛŒÚ† Ú©Ø±Ø¯
4. âœ… **Cost-effective:** ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ ÙˆØ§Ù‚Ø¹ÛŒ Ù¾ÙˆÙ„ Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ…

---

## ğŸš€ **Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø± SecureRedLab**

### **ÙØ§ÛŒÙ„ Ø¬Ø¯ÛŒØ¯: `backend/ai_intelligence/model_config.py`**

```python
from enum import Enum
from typing import Dict, Optional
import os

class ModelDeployment(Enum):
    ONLINE_API = "online"
    OFFLINE_LOCAL = "offline"

class AIModelConfig:
    """
    Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ AI
    """
    
    # VLM Models
    VLM_MODELS = {
        "qwen_2_5_vl_72b": {
            "deployment": ModelDeployment.ONLINE_API,
            "api_endpoint": "https://dashscope.aliyuncs.com/api/v1/services/aigc/multimodal-generation/generation",
            "api_key_env": "QWEN_API_KEY",
            "cost_per_1m_tokens": 5.0,  # Average $3-8
            "max_tokens": 8192,
            "supports_vision": True
        },
        "gemini_2_flash": {
            "deployment": ModelDeployment.ONLINE_API,
            "api_endpoint": "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent",
            "api_key_env": "GEMINI_API_KEY",
            "cost_per_1m_tokens": 0.075,
            "max_tokens": 1000000,  # 1M context!
            "supports_vision": True
        },
        "claude_3_5_sonnet": {
            "deployment": ModelDeployment.ONLINE_API,
            "api_endpoint": "https://api.anthropic.com/v1/messages",
            "api_key_env": "CLAUDE_API_KEY",
            "cost_per_1m_tokens": 3.0,
            "max_tokens": 200000,
            "supports_vision": True
        },
        "llava_1_6_34b": {
            "deployment": ModelDeployment.OFFLINE_LOCAL,
            "model_path": "liuhaotian/llava-v1.6-34b",
            "ollama_name": "llava:34b-v1.6",
            "vram_required_gb": 48,
            "cost_per_1m_tokens": 0.0,
            "max_tokens": 4096,
            "supports_vision": True
        },
        "llava_1_6_13b": {
            "deployment": ModelDeployment.OFFLINE_LOCAL,
            "model_path": "liuhaotian/llava-v1.6-13b",
            "ollama_name": "llava:13b-v1.6",
            "vram_required_gb": 16,
            "cost_per_1m_tokens": 0.0,
            "max_tokens": 4096,
            "supports_vision": True
        }
    }
    
    # LLM Models
    LLM_MODELS = {
        "glm_4_6": {
            "deployment": ModelDeployment.ONLINE_API,
            "api_endpoint": "https://open.bigmodel.cn/api/paas/v4/chat/completions",
            "api_key_env": "GLM_API_KEY",
            "cost_per_1m_tokens": 0.55,  # Average $0.3-0.8
            "max_tokens": 128000,
            "supports_vision": False,
            "note": "Best value - 93.9% AIME vs Claude's 74.3%"
        },
        "deepseek_coder_v2": {
            "deployment": ModelDeployment.ONLINE_API,
            "api_endpoint": "https://api.deepseek.com/v1/chat/completions",
            "api_key_env": "DEEPSEEK_API_KEY",
            "cost_per_1m_tokens": 0.14,
            "max_tokens": 128000,
            "supports_vision": False
        },
        "qwen_2_5_72b": {
            "deployment": ModelDeployment.ONLINE_API,
            "api_endpoint": "https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation",
            "api_key_env": "QWEN_API_KEY",
            "cost_per_1m_tokens": 0.44,  # Average $0.17-0.70
            "max_tokens": 131072,
            "supports_vision": False
        },
        "mixtral_8x22b": {
            "deployment": ModelDeployment.ONLINE_API,
            "api_endpoint": "https://api.mistral.ai/v1/chat/completions",
            "api_key_env": "MISTRAL_API_KEY",
            "cost_per_1m_tokens": 0.9,
            "max_tokens": 64000,
            "supports_vision": False
        },
        "llama_3_1_70b": {
            "deployment": ModelDeployment.OFFLINE_LOCAL,
            "model_path": "meta-llama/Llama-3.1-70B-Instruct",
            "ollama_name": "llama3.1:70b",
            "vram_required_gb": 80,
            "cost_per_1m_tokens": 0.0,
            "max_tokens": 128000,
            "supports_vision": False
        },
        "llama_3_1_8b": {
            "deployment": ModelDeployment.OFFLINE_LOCAL,
            "model_path": "meta-llama/Llama-3.1-8B-Instruct",
            "ollama_name": "llama3.1:8b",
            "vram_required_gb": 12,
            "cost_per_1m_tokens": 0.0,
            "max_tokens": 128000,
            "supports_vision": False
        }
    }
    
    @classmethod
    def get_model_config(cls, model_name: str) -> Optional[Dict]:
        """Get configuration for a specific model"""
        # Check VLM models
        if model_name in cls.VLM_MODELS:
            return cls.VLM_MODELS[model_name]
        # Check LLM models
        if model_name in cls.LLM_MODELS:
            return cls.LLM_MODELS[model_name]
        return None
    
    @classmethod
    def get_online_models(cls) -> Dict[str, Dict]:
        """Get all online API models"""
        online = {}
        for name, config in {**cls.VLM_MODELS, **cls.LLM_MODELS}.items():
            if config["deployment"] == ModelDeployment.ONLINE_API:
                online[name] = config
        return online
    
    @classmethod
    def get_offline_models(cls) -> Dict[str, Dict]:
        """Get all offline local models"""
        offline = {}
        for name, config in {**cls.VLM_MODELS, **cls.LLM_MODELS}.items():
            if config["deployment"] == ModelDeployment.OFFLINE_LOCAL:
                offline[name] = config
        return offline
```

---

## ğŸ“ **Recommendation Table**

| Use Case | Recommended Models | Reason |
|----------|-------------------|--------|
| **Visual Analysis** | Qwen2.5-VL â†’ Gemini 2.0 Flash | Best VLM, Fastest |
| **Code Analysis** | Claude 3.5 Sonnet â†’ GLM-4.6 | Best for code |
| **Payload Generation** | GLM-4.6 â†’ DeepSeek Coder V2 | Best value, Cheap |
| **Evasion Techniques** | DeepSeek Coder V2 â†’ GLM-4.6 | Exploit specialist |
| **Offline/Privacy** | LLaVA 1.6-13B + LLaMA 3.1-8B | No GPU? Use 13B+8B |
| **Academic/Research** | Offline Models | Full control, No data sharing |

---

## ğŸ”’ **Security & Privacy Notes**

### **Online APIs:**
- âš ï¸ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ servers Ø§Ø±Ø³Ø§Ù„ Ù…ÛŒâ€ŒØ´ÙˆØ¯
- âœ… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² HTTPS
- âœ… API keys Ø¨Ø§ÛŒØ¯ encrypted Ø¨Ø§Ø´Ù†Ø¯
- âš ï¸ Ù†Ø¨Ø§ÛŒØ¯ sensitive data Ø§Ø±Ø³Ø§Ù„ Ø´ÙˆØ¯

### **Offline Models:**
- âœ… 100% Privacy
- âœ… No data leakage
- âœ… Perfect for academic research
- âŒ Ù†ÛŒØ§Ø² Ø¨Ù‡ GPU Ù‚ÙˆÛŒ

---

## ğŸ“Š **Benchmark Summary (2025)**

| Model | MMLU | HumanEval | MATH | Cost/1M |
|-------|------|-----------|------|---------|
| **GLM-4.6** | 85.5% | 75.2% | 93.9% AIME | $0.55 |
| **Claude 3.5 Sonnet** | 88.3% | 92.0% | 74.3% AIME | $3.00 |
| **DeepSeek Coder V2** | 78.9% | 90.2% | 75.7% | $0.14 |
| **Qwen2.5-VL-72B** | 86.5% | 70.8% | 80.5% | $5.00 |
| **Gemini 2.0 Flash** | 84.0% | 75.0% | 78.0% | $0.075 |

**ğŸ’¡ Ù†ØªÛŒØ¬Ù‡:** GLM-4.6 Ø¯Ø± MATH Ø¨Ù‡ØªØ±ÛŒÙ† Ø§Ø³Øª ÙˆÙ„ÛŒ Claude Ø¯Ø± code Ø¨Ù‡ØªØ± Ø§Ø³Øª.

---

**ğŸš€ Ø§Ø¯Ø§Ù…Ù‡ Ø¯Ø§Ø±Ø¯...**
